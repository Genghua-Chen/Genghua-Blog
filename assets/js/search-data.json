{
  
    
        "post0": {
            "title": "Mobile Games A/B Testing",
            "content": "import numpy as np import pandas as pd import scipy.stats as stats import statsmodels.stats.api as sms import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from math import ceil from statsmodels.stats.proportion import proportions_ztest, proportion_confint %matplotlib inline # Some plot styling preferences plt.style.use(&#39;seaborn-whitegrid&#39;) font = {&#39;family&#39; : &#39;Helvetica&#39;, &#39;weight&#39; : &#39;bold&#39;, &#39;size&#39; : 14} mpl.rc(&#39;font&#39;, **font) effect_size = sms.proportion_effectsize(0.13, 0.15) # Calculating effect size based on our expected rates required_n = sms.NormalIndPower().solve_power( effect_size, power=0.8, alpha=0.05, ratio=1 ) # Calculating sample size needed required_n = ceil(required_n) # Rounding up to next whole number print(required_n) . 4720 . H0: p = p0 . Ha: p != p0 . df = pd.read_csv(&#39;/Users/genghua/Downloads/cookie_cats.csv&#39;) df.head() # https://www.kaggle.com/yufengsui/mobile-games-ab-testing . userid version sum_gamerounds retention_1 retention_7 . 0 | 116 | gate_30 | 3 | False | False | . 1 | 337 | gate_30 | 38 | True | False | . 2 | 377 | gate_40 | 165 | True | False | . 3 | 483 | gate_40 | 1 | False | False | . 4 | 488 | gate_40 | 179 | True | True | . df.shape . (90189, 5) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 90189 entries, 0 to 90188 Data columns (total 5 columns): userid 90189 non-null int64 version 90189 non-null object sum_gamerounds 90189 non-null int64 retention_1 90189 non-null bool retention_7 90189 non-null bool dtypes: bool(2), int64(2), object(1) memory usage: 2.2+ MB . # df = df.replace(to_replace =&quot;False&quot;, value =0) result = df[&#39;retention_1&#39;].astype(int) df[&#39;retention_1&#39;] = result result = df[&#39;retention_7&#39;].astype(int) df[&#39;retention_7&#39;] = result . df . userid version sum_gamerounds retention_1 retention_7 . 0 | 116 | gate_30 | 3 | 0 | 0 | . 1 | 337 | gate_30 | 38 | 1 | 0 | . 2 | 377 | gate_40 | 165 | 1 | 0 | . 3 | 483 | gate_40 | 1 | 0 | 0 | . 4 | 488 | gate_40 | 179 | 1 | 1 | . ... | ... | ... | ... | ... | ... | . 90184 | 9999441 | gate_40 | 97 | 1 | 0 | . 90185 | 9999479 | gate_40 | 30 | 0 | 0 | . 90186 | 9999710 | gate_30 | 28 | 1 | 0 | . 90187 | 9999768 | gate_40 | 51 | 1 | 0 | . 90188 | 9999861 | gate_40 | 16 | 0 | 0 | . 90189 rows × 5 columns . Sampling . gate30_sample = df[df[&#39;version&#39;] == &#39;gate_30&#39;].sample(n=required_n, random_state=22) gate40_sample = df[df[&#39;version&#39;] == &#39;gate_40&#39;].sample(n=required_n, random_state=22) ab_test = pd.concat([gate30_sample, gate40_sample], axis=0) ab_test.reset_index(drop=True, inplace=True) ab_test . userid version sum_gamerounds retention_1 retention_7 . 0 | 7540471 | gate_30 | 45 | 1 | 0 | . 1 | 3589138 | gate_30 | 21 | 1 | 0 | . 2 | 3177668 | gate_30 | 14 | 1 | 0 | . 3 | 2133884 | gate_30 | 26 | 0 | 0 | . 4 | 492763 | gate_30 | 39 | 1 | 1 | . ... | ... | ... | ... | ... | ... | . 9435 | 8267302 | gate_40 | 27 | 0 | 0 | . 9436 | 7145315 | gate_40 | 113 | 1 | 1 | . 9437 | 396129 | gate_40 | 20 | 1 | 0 | . 9438 | 2165928 | gate_40 | 0 | 0 | 0 | . 9439 | 2888784 | gate_40 | 5 | 0 | 0 | . 9440 rows × 5 columns . ab_test[&#39;version&#39;].value_counts() . gate_30 4720 gate_40 4720 Name: version, dtype: int64 . Visualising the results . fig, ax = plt.subplots(figsize=(5,5)) ab_test[&#39;retention_1&#39;].value_counts().plot.pie(explode = [0.1,0], autopct=&#39;%1.1f%%&#39;, shadow = True) ax.set_title(&#39;retention_1&#39;) ax.set_ylabel(&#39; &#39;) plt.show() . fig, ax = plt.subplots(figsize=(5,5)) ab_test[&#39;retention_7&#39;].value_counts().plot.pie(explode = [0.1,0], autopct=&#39;%1.1f%%&#39;, shadow = True) ax.set_title(&#39;retention_7&#39;) ax.set_ylabel(&#39; &#39;) plt.show() . sns.countplot(x=&#39;version&#39;, hue=&#39;retention_1&#39;, data=ab_test) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc2d047b990&gt; . sns.countplot(x=&#39;version&#39;, hue=&#39;retention_7&#39;, data=ab_test) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc2d04de190&gt; . conversion_rates = ab_test.groupby(&#39;version&#39;)[&#39;retention_1&#39;] std_p = lambda x: np.std(x, ddof=0) # Std. deviation of the proportion se_p = lambda x: stats.sem(x, ddof=0) # Std. error of the proportion (std / sqrt(n)) conversion_rates = conversion_rates.agg([np.mean, std_p, se_p]) conversion_rates.columns = [&#39;retention_1_rate&#39;, &#39;std_deviation&#39;, &#39;std_error&#39;] conversion_rates.style.format(&#39;{:.3f}&#39;) . retention_1_rate std_deviation std_error . version . gate_30 0.445 | 0.497 | 0.007 | . gate_40 0.433 | 0.496 | 0.007 | . Testing the hypothesis . gate_30_results = ab_test[ab_test[&#39;version&#39;] == &#39;gate_30&#39;][&#39;retention_1&#39;] gate_40_results = ab_test[ab_test[&#39;version&#39;] == &#39;gate_40&#39;][&#39;retention_1&#39;] n_30 = gate_30_results.count() n_40 = gate_40_results.count() successes = [gate_30_results.sum(), gate_40_results.sum()] nobs = [n_30, n_40] z_stat, pval = proportions_ztest(successes, nobs=nobs) (lower_30, lower_40), (upper_30, upper_40) = proportion_confint(successes, nobs=nobs, alpha=0.05) print(f&#39;Rejection Region: {z_stat:.2f}&#39;) print(f&#39;p-value: {pval:.3f}&#39;) print(f&#39;Confident Interval 95% for Gate_30 version: [{lower_30:.3f}, {upper_30:.3f}]&#39;) print(f&#39;Confident Interval 95% for Gate_40 version: [{lower_40:.3f}, {upper_40:.3f}]&#39;) . Rejection Region: 1.16 p-value: 0.246 Confident Interval 95% for Gate_30 version: [0.431, 0.460] Confident Interval 95% for Gate_40 version: [0.419, 0.448] . We fail to reject H0 since p-value is not in rejection region .",
            "url": "https://genghua-chen.github.io/Genghua-Blog/2021/04/13/Mobile-Games.html",
            "relUrl": "/2021/04/13/Mobile-Games.html",
            "date": " • Apr 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Genghua Chen Resume",
            "content": "Education . Feitian College at Middletown, Middletown, NY Expected June 2022 . Bachelor of Data Science . Stratford High School, Houston, TX May 2018 . PROJECTS . Bank Marketing Deposit Classification January 2021 - February 2021 . Conducted data cleaning, converted categorical to numeric and selected statistical significance features to improve model performance and performed more than 93% area under the ROC curve score for the model predicting power . Tested multiple classification models such as Random Forest, Decision Tree, K-nearest neighbors, Extra-Trees and Gradient Boosting . World happiness  June 2020 - August 2020 . Business data analytics . Imported structure table, CSV and  Excel files into Tableau and designed and built a data visualization dashboard using two years historical data to rank most happiness and unhappiness . Built a heat map, scatter plots and pie chart to display six key factors: income, freedom, trust in government, life expectancy, social support and generosity . Presented PowerPoint presentation to clearly communicated world happiness result . Experience . The Epoch Times, New York, NY  July 2019 - September 2019 . Advertising Sales Assistant . Monitored subscribers and emerging trends in Google Analytics and effectively defined target market . Collaborated with leadership to prepared a list of targeted companies for e-blast . Raised brand awareness via e-blast to increase number of subscribers revenue and profit . New Tang Dynasty Television, Houston, TX March 2017 - November 2017 . Video Editor . Created an initial proposed storyboard draft of more than 30 videos to presented to stakeholders’ and received approvals before production to digitally alter video . Analyzed written scripts and raw materials to create a timeline based on valuable scenes and contribution . Continuously discovered and implemented new editing technologies and industry’s best practices to maximize efficiency . Kate spade outlets, Woodbury, NY November 2019 - January 2020 . Sales Associate . Results-oriented, effectively cross-selling products, and increase 15% sales revenue and 35% customer service by leveraging my communication, bilingual literacy and by analyzing foot traffic at the plaza . Assessed customers’ needs, introduced promotions and products, for order management and improved engagement and maintained customer relations .",
            "url": "https://genghua-chen.github.io/Genghua-Blog/2021/03/01/Genghua-Chen-Resume.html",
            "relUrl": "/2021/03/01/Genghua-Chen-Resume.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Bank Marketing Data Analysis and Classification",
            "content": "0.The Dataset . Age . Job . Marital Status . Education . Default: Having a previously broken credit . Balance: Balance of the individual. . Housing: Any housing loan . Loan: Any personal Loan . Contact: Contact communication type . Month: Last month of contact . Day: The day of the contacted . Duration: Talk time on last call(Unit: Sec) . Campaign: The number of contacts reaching the customer during the current campaign . Pdays: The number of days since the previous campaign, if reached (-1 if it was never reached before) . Previous: The number of contacts that reached the customer before this campaign . Poutcome: Previous campaign success, failure or failure . Deposit: Client subscribed a term deposit . 1.Exploratory data analysis . import numpy as np import pandas as pd import copy # conda install -c https://conda.anaconda.org/plotly plotly import matplotlib.pyplot as plt import matplotlib.style as style import numpy as np import pandas as pd import plotly.express as px import seaborn as sns from scipy import stats import warnings warnings.filterwarnings(&quot;ignore&quot;) import seaborn as sns import matplotlib.pyplot as plt from matplotlib import pyplot from datetime import date from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import Normalizer from sklearn.preprocessing import MinMaxScaler from numpy import array from numpy import argmax from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split from collections import defaultdict from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestClassifier from sklearn import metrics from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve from sklearn.metrics import accuracy_score from sklearn.tree import DecisionTreeClassifier from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble.forest import ExtraTreesClassifier from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier from sklearn.metrics import confusion_matrix,auc,roc_curve # from lightgbm import LGBMClassifier from sklearn.neighbors import KNeighborsClassifier import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . df = pd.read_csv(&#39;/Users/genghua/Desktop/Data/bank.csv&#39;) . df.head(5) . age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome deposit . 0 | 59 | admin. | married | secondary | no | 2343 | yes | no | unknown | 5 | may | 1042 | 1 | -1 | 0 | unknown | yes | . 1 | 56 | admin. | married | secondary | no | 45 | no | no | unknown | 5 | may | 1467 | 1 | -1 | 0 | unknown | yes | . 2 | 41 | technician | married | secondary | no | 1270 | yes | no | unknown | 5 | may | 1389 | 1 | -1 | 0 | unknown | yes | . 3 | 55 | services | married | secondary | no | 2476 | yes | no | unknown | 5 | may | 579 | 1 | -1 | 0 | unknown | yes | . 4 | 54 | admin. | married | tertiary | no | 184 | no | no | unknown | 5 | may | 673 | 2 | -1 | 0 | unknown | yes | . df.shape . (11162, 17) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 11162 entries, 0 to 11161 Data columns (total 17 columns): age 11162 non-null int64 job 11162 non-null object marital 11162 non-null object education 11162 non-null object default 11162 non-null object balance 11162 non-null int64 housing 11162 non-null object loan 11162 non-null object contact 11162 non-null object day 11162 non-null int64 month 11162 non-null object duration 11162 non-null int64 campaign 11162 non-null int64 pdays 11162 non-null int64 previous 11162 non-null int64 poutcome 11162 non-null object deposit 11162 non-null object dtypes: int64(7), object(10) memory usage: 1.4+ MB . df.isnull().sum() . age 0 job 0 marital 0 education 0 default 0 balance 0 housing 0 loan 0 contact 0 day 0 month 0 duration 0 campaign 0 pdays 0 previous 0 poutcome 0 deposit 0 dtype: int64 . df.describe() . age balance day duration campaign pdays previous . count | 11162.000000 | 11162.000000 | 11162.000000 | 11162.000000 | 11162.000000 | 11162.000000 | 11162.000000 | . mean | 41.231948 | 1528.538524 | 15.658036 | 371.993818 | 2.508421 | 51.330407 | 0.832557 | . std | 11.913369 | 3225.413326 | 8.420740 | 347.128386 | 2.722077 | 108.758282 | 2.292007 | . min | 18.000000 | -6847.000000 | 1.000000 | 2.000000 | 1.000000 | -1.000000 | 0.000000 | . 25% | 32.000000 | 122.000000 | 8.000000 | 138.000000 | 1.000000 | -1.000000 | 0.000000 | . 50% | 39.000000 | 550.000000 | 15.000000 | 255.000000 | 2.000000 | -1.000000 | 0.000000 | . 75% | 49.000000 | 1708.000000 | 22.000000 | 496.000000 | 3.000000 | 20.750000 | 1.000000 | . max | 95.000000 | 81204.000000 | 31.000000 | 3881.000000 | 63.000000 | 854.000000 | 58.000000 | . df.describe(include = &#39;object&#39;) . job marital education default housing loan contact month poutcome deposit . count | 11162 | 11162 | 11162 | 11162 | 11162 | 11162 | 11162 | 11162 | 11162 | 11162 | . unique | 12 | 3 | 4 | 2 | 2 | 2 | 3 | 12 | 4 | 2 | . top | management | married | secondary | no | no | no | cellular | may | unknown | no | . freq | 2566 | 6351 | 5476 | 10994 | 5881 | 9702 | 8042 | 2824 | 8326 | 5873 | . df[&#39;deposit&#39;].value_counts() . no 5873 yes 5289 Name: deposit, dtype: int64 . # ndf = df.loc[(df[&#39;poutcome&#39;] != &quot;unknown&quot;) &amp; (df[&#39;poutcome&#39;] != &quot;other&quot;)] # print(df.shape, ndf.shape) . counts = df.deposit.value_counts() normal = counts[0] Churn = counts[1] perc_normal = (normal/(normal+Churn))*100 perc_Churn = (Churn/(normal+Churn))*100 print(&#39;There were {} non-Churn ({:.3f}%) and {} Churn ({:.3f}%).&#39;.format(normal, perc_normal, Churn, perc_Churn)) . There were 5873 non-Churn (52.616%) and 5289 Churn (47.384%). . df . age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome deposit . 0 | 59 | admin. | married | secondary | no | 2343 | yes | no | unknown | 5 | may | 1042 | 1 | -1 | 0 | unknown | yes | . 1 | 56 | admin. | married | secondary | no | 45 | no | no | unknown | 5 | may | 1467 | 1 | -1 | 0 | unknown | yes | . 2 | 41 | technician | married | secondary | no | 1270 | yes | no | unknown | 5 | may | 1389 | 1 | -1 | 0 | unknown | yes | . 3 | 55 | services | married | secondary | no | 2476 | yes | no | unknown | 5 | may | 579 | 1 | -1 | 0 | unknown | yes | . 4 | 54 | admin. | married | tertiary | no | 184 | no | no | unknown | 5 | may | 673 | 2 | -1 | 0 | unknown | yes | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 11157 | 33 | blue-collar | single | primary | no | 1 | yes | no | cellular | 20 | apr | 257 | 1 | -1 | 0 | unknown | no | . 11158 | 39 | services | married | secondary | no | 733 | no | no | unknown | 16 | jun | 83 | 4 | -1 | 0 | unknown | no | . 11159 | 32 | technician | single | secondary | no | 29 | no | no | cellular | 19 | aug | 156 | 2 | -1 | 0 | unknown | no | . 11160 | 43 | technician | married | secondary | no | 0 | no | yes | cellular | 8 | may | 9 | 2 | 172 | 5 | failure | no | . 11161 | 34 | technician | married | secondary | no | 0 | no | no | cellular | 9 | jul | 628 | 1 | -1 | 0 | unknown | no | . 11162 rows × 17 columns . # df.job.value_counts()/ len(df.job)*100 # df.marital.value_counts()/ len(df.marital)*100 # df.education.value_counts()/ len(df.education)*100 # df.default.value_counts()/ len(df.default)*100 # df.loan.value_counts()/ len(df.loan)*100 # df.contact.value_counts()/ len(df.contact)*100 # df.month.value_counts()/ len(df.month)*100 # df.poutcome.value_counts()/ len(df.duration)*100 # df.deposit.value_counts()/ len(df.duration)*100 . df.hist(figsize=(14,14)) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e63ce8d50&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e63b61bd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e647dbb50&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e640a2e90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e63c13c10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e64358f10&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e63e47790&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e63f6ca90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9e63f88550&gt;]], dtype=object) . bal_his = df.loc[(df[&#39;balance&#39;] &lt; 20000)] bal_his[&#39;balance&#39;].hist(figsize=(10,10), bins = 40) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9e63a97b10&gt; . f, axes = plt.subplots(nrows=2, ncols=3, figsize=(25,15)) sns.boxplot(x=&quot;deposit&quot;, y=&quot;age&quot;, data=df, ax=axes[0,0]) sns.boxplot(x=&quot;deposit&quot;, y=&quot;balance&quot;, data=df, ax=axes[0,1]) sns.boxplot(x=&quot;deposit&quot;, y=&quot;day&quot;, data=df, ax=axes[0,2]) sns.boxplot(x=&quot;deposit&quot;, y=&quot;duration&quot;, data=df, ax=axes[1,0]) sns.boxplot(x=&quot;deposit&quot;, y=&quot;campaign&quot;, data=df, ax=axes[1,1]) sns.boxplot(x=&quot;deposit&quot;, y=&quot;pdays&quot;, data=df, ax=axes[1,2]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9e611b8750&gt; . plt.figure(figsize=[12,14]) features=[&quot;marital&quot;, &quot;education&quot;, &quot;contact&quot;, &quot;default&quot;, &quot;housing&quot;, &quot;loan&quot;, &quot;poutcome&quot;] n=1 for f in features: plt.subplot(4,2,n) sns.countplot(x=f, hue=&#39;deposit&#39;, edgecolor=&quot;black&quot;, alpha=0.7, data=df) sns.despine() plt.title(&quot;Countplot of {} by deposit&quot;.format(f)) n=n+1 plt.tight_layout() plt.show() plt.figure(figsize=[14,4]) sns.countplot(x=&#39;job&#39;, hue=&#39;deposit&#39;,edgecolor=&quot;black&quot;, alpha=0.7, data=df) sns.despine() plt.title(&quot;Countplot of job by deposit&quot;) plt.show() . sns.set_style(&#39;whitegrid&#39;) plt.subplots(figsize = (10,10)) ## Plotting heatmap. Generate a mask for the upper triangle (taken from seaborn example gallery) mask = np.zeros_like(df.corr(), dtype=np.bool) mask[np.triu_indices_from(mask)] = True ax = sns.heatmap(df.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, ); plt.title(&quot;Heatmap of Data Set&quot;, fontsize = 25); ax.get_ylim() # output: (6.5, 0.5) ax.set_ylim(7,0) . (7, 0) . 2.Converting Categorical Columns to Numeric Columns . # df=pd.concat([df,pd.get_dummies(df[columns])],axis=1) # df=df.drop(df[&#39;job&#39;,&#39;marital&#39;,&#39;education&#39;,&#39;default&#39;,&#39;housing&#39;,&#39;loan&#39;,&#39;contact&#39;,&#39;month&#39;,&#39;day&#39;,&#39;poutcome&#39;],axis=1) df = df.replace(to_replace =&quot;yes&quot;, value =1) df = df.replace(to_replace =&quot;no&quot;, value =0) . df.dtypes . age int64 job object marital object education object default int64 balance int64 housing int64 loan int64 contact object day int64 month object duration int64 campaign int64 pdays int64 previous int64 poutcome object deposit int64 dtype: object . df . age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome deposit . 0 | 59 | admin. | married | secondary | 0 | 2343 | 1 | 0 | unknown | 5 | may | 1042 | 1 | -1 | 0 | unknown | 1 | . 1 | 56 | admin. | married | secondary | 0 | 45 | 0 | 0 | unknown | 5 | may | 1467 | 1 | -1 | 0 | unknown | 1 | . 2 | 41 | technician | married | secondary | 0 | 1270 | 1 | 0 | unknown | 5 | may | 1389 | 1 | -1 | 0 | unknown | 1 | . 3 | 55 | services | married | secondary | 0 | 2476 | 1 | 0 | unknown | 5 | may | 579 | 1 | -1 | 0 | unknown | 1 | . 4 | 54 | admin. | married | tertiary | 0 | 184 | 0 | 0 | unknown | 5 | may | 673 | 2 | -1 | 0 | unknown | 1 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 11157 | 33 | blue-collar | single | primary | 0 | 1 | 1 | 0 | cellular | 20 | apr | 257 | 1 | -1 | 0 | unknown | 0 | . 11158 | 39 | services | married | secondary | 0 | 733 | 0 | 0 | unknown | 16 | jun | 83 | 4 | -1 | 0 | unknown | 0 | . 11159 | 32 | technician | single | secondary | 0 | 29 | 0 | 0 | cellular | 19 | aug | 156 | 2 | -1 | 0 | unknown | 0 | . 11160 | 43 | technician | married | secondary | 0 | 0 | 0 | 1 | cellular | 8 | may | 9 | 2 | 172 | 5 | failure | 0 | . 11161 | 34 | technician | married | secondary | 0 | 0 | 0 | 0 | cellular | 9 | jul | 628 | 1 | -1 | 0 | unknown | 0 | . 11162 rows × 17 columns . sns.set_style(&#39;whitegrid&#39;) plt.subplots(figsize = (10,10)) ## Plotting heatmap. Generate a mask for the upper triangle (taken from seaborn example gallery) mask = np.zeros_like(df.corr(), dtype=np.bool) mask[np.triu_indices_from(mask)] = True ax = sns.heatmap(df.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, ); plt.title(&quot;Heatmap of New Data Set&quot;, fontsize = 25); ax.get_ylim() # output: (10.5, 0.5) ax.set_ylim(11,0) . (11, 0) . cols=np.array(df.columns[df.dtypes != object]) df.dtypes # cols # for i in df_train.columns: # if i not in cols: # df_train[i]=df_train[i].map(str) # df_train.drop(columns=cols,inplace=True) # df_train.head(5) . age int64 job object marital object education object default int64 balance int64 housing int64 loan int64 contact object day int64 month object duration int64 campaign int64 pdays int64 previous int64 poutcome object deposit int64 dtype: object . # # random.seed(2) # cols=np.array(df.columns[df.dtypes != object]) # d = defaultdict(LabelEncoder) # df_train = df_train.apply(lambda x: d[x.name].fit_transform(x)) # df_train[cols] = df[cols] # df_train.head(5) . df_train = df.copy() df_train = pd.get_dummies(df_train, columns = [&#39;job&#39;, &#39;marital&#39;, &#39;education&#39;, &#39;contact&#39;, &#39;month&#39;, &#39;poutcome&#39;]) df_train df_train.dtypes . age int64 default int64 balance int64 housing int64 loan int64 day int64 duration int64 campaign int64 pdays int64 previous int64 deposit int64 job_admin. uint8 job_blue-collar uint8 job_entrepreneur uint8 job_housemaid uint8 job_management uint8 job_retired uint8 job_self-employed uint8 job_services uint8 job_student uint8 job_technician uint8 job_unemployed uint8 job_unknown uint8 marital_divorced uint8 marital_married uint8 marital_single uint8 education_primary uint8 education_secondary uint8 education_tertiary uint8 education_unknown uint8 contact_cellular uint8 contact_telephone uint8 contact_unknown uint8 month_apr uint8 month_aug uint8 month_dec uint8 month_feb uint8 month_jan uint8 month_jul uint8 month_jun uint8 month_mar uint8 month_may uint8 month_nov uint8 month_oct uint8 month_sep uint8 poutcome_failure uint8 poutcome_other uint8 poutcome_success uint8 poutcome_unknown uint8 dtype: object . # plt.subplots(figsize = (20,20)) # ## Plotting heatmap. Generate a mask for the upper triangle (taken from seaborn example gallery) # mask = np.zeros_like(df_train.corr(), dtype=np.bool) # mask[np.triu_indices_from(mask)] = True # ax = sns.heatmap(df_train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, ); # plt.title(&quot;Heatmap of All Data Set&quot;, fontsize = 25); # ax.get_ylim() # output: (16.5, 0.5) # ax.set_ylim(17,0) . 3.Feature Selection . corr = df_train.corr() a = corr.sort_values([&quot;deposit&quot;], ascending = False, inplace = True) print(corr.deposit) . deposit 1.000000 duration 0.451919 poutcome_success 0.286642 contact_cellular 0.223252 pdays 0.151593 previous 0.139867 month_mar 0.135438 month_oct 0.133783 month_sep 0.126901 job_retired 0.103827 job_student 0.099953 marital_single 0.094632 education_tertiary 0.094598 month_apr 0.090975 month_dec 0.086964 balance 0.081129 month_feb 0.051710 poutcome_other 0.044059 job_management 0.036301 age 0.034901 job_unemployed 0.033487 poutcome_failure 0.020714 contact_telephone 0.016420 education_unknown 0.014355 marital_divorced 0.005228 job_unknown 0.001889 job_admin. -0.000610 job_self-employed -0.004707 job_technician -0.011557 month_aug -0.016621 month_jun -0.018982 month_jan -0.021803 job_housemaid -0.024155 month_nov -0.028278 job_entrepreneur -0.034443 default -0.040680 job_services -0.044531 month_jul -0.047368 education_secondary -0.051952 day -0.056326 education_primary -0.063002 marital_married -0.092157 job_blue-collar -0.100840 loan -0.110580 campaign -0.128081 month_may -0.170507 housing -0.203888 poutcome_unknown -0.230470 contact_unknown -0.256136 Name: deposit, dtype: float64 . c = corr[[&#39;deposit&#39;]] c d = corr[&#39;deposit&#39;].loc[(corr[&#39;deposit&#39;].abs() &gt; 0.1)] print(d) type(d) list(d.index) . deposit 1.000000 duration 0.451919 poutcome_success 0.286642 contact_cellular 0.223252 pdays 0.151593 previous 0.139867 month_mar 0.135438 month_oct 0.133783 month_sep 0.126901 job_retired 0.103827 job_blue-collar -0.100840 loan -0.110580 campaign -0.128081 month_may -0.170507 housing -0.203888 poutcome_unknown -0.230470 contact_unknown -0.256136 Name: deposit, dtype: float64 . [&#39;deposit&#39;, &#39;duration&#39;, &#39;poutcome_success&#39;, &#39;contact_cellular&#39;, &#39;pdays&#39;, &#39;previous&#39;, &#39;month_mar&#39;, &#39;month_oct&#39;, &#39;month_sep&#39;, &#39;job_retired&#39;, &#39;job_blue-collar&#39;, &#39;loan&#39;, &#39;campaign&#39;, &#39;month_may&#39;, &#39;housing&#39;, &#39;poutcome_unknown&#39;, &#39;contact_unknown&#39;] . df_g1 = df_train[[&#39;deposit&#39;, &#39;duration&#39;, &#39;poutcome_success&#39;, &#39;contact_cellular&#39;, &#39;pdays&#39;, &#39;previous&#39;, &#39;month_mar&#39;, &#39;month_oct&#39;, &#39;month_sep&#39;, &#39;job_retired&#39;, &#39;loan&#39;, &#39;campaign&#39;, &#39;month_may&#39;, &#39;housing&#39;, &#39;poutcome_unknown&#39;, &#39;contact_unknown&#39;]] . df_g2 = df_train[list(d.index)] df_g2 . deposit duration poutcome_success contact_cellular pdays previous month_mar month_oct month_sep job_retired job_blue-collar loan campaign month_may housing poutcome_unknown contact_unknown . 0 | 1 | 1042 | 0 | 0 | -1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | . 1 | 1 | 1467 | 0 | 0 | -1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 | . 2 | 1 | 1389 | 0 | 0 | -1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | . 3 | 1 | 579 | 0 | 0 | -1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | . 4 | 1 | 673 | 0 | 0 | -1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 1 | 1 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 11157 | 0 | 257 | 0 | 1 | -1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | . 11158 | 0 | 83 | 0 | 0 | -1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 1 | 1 | . 11159 | 0 | 156 | 0 | 1 | -1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 1 | 0 | . 11160 | 0 | 9 | 0 | 1 | 172 | 5 | 0 | 0 | 0 | 0 | 0 | 1 | 2 | 1 | 0 | 0 | 0 | . 11161 | 0 | 628 | 0 | 1 | -1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | . 11162 rows × 17 columns . sns.set_style(&#39;whitegrid&#39;) plt.subplots(figsize = (15,15)) ## Plotting heatmap. Generate a mask for the upper triangle (taken from seaborn example gallery) mask = np.zeros_like(df_g2.corr(), dtype=np.bool) mask[np.triu_indices_from(mask)] = True ax = sns.heatmap(df_g2.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, ); plt.title(&quot;Heatmap of New Data Set&quot;, fontsize = 25); ax.get_ylim() # output: (10.5, 0.5) ax.set_ylim(17,0) . (17, 0) . . x = df_train.duration y = df_train.deposit plt.scatter(x, y) model = LinearRegression(fit_intercept=True) model.fit(x[:, np.newaxis], y) xfit = np.linspace(0, 4000, 1000) yfit = model.predict(xfit[:, np.newaxis]) plt.scatter(x, y) plt.plot(xfit, yfit) . [&lt;matplotlib.lines.Line2D at 0x7f9e625ec810&gt;] . train, test = train_test_split(df_train, test_size=0.3, random_state=42, shuffle=True) print(train.shape, test.shape) . (7813, 49) (3349, 49) . train.describe() . age default balance housing loan day duration campaign pdays previous ... month_jun month_mar month_may month_nov month_oct month_sep poutcome_failure poutcome_other poutcome_success poutcome_unknown . count | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | ... | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | 7813.000000 | . mean | 41.237169 | 0.014335 | 1536.787278 | 0.467938 | 0.133495 | 15.605273 | 369.986433 | 2.499552 | 49.952643 | 0.802253 | ... | 0.109177 | 0.024318 | 0.252400 | 0.087290 | 0.036094 | 0.029566 | 0.108665 | 0.045565 | 0.094714 | 0.751056 | . std | 11.850243 | 0.118876 | 3215.999050 | 0.499003 | 0.340131 | 8.419817 | 345.882463 | 2.722798 | 107.826919 | 2.135139 | ... | 0.311881 | 0.154046 | 0.434417 | 0.282278 | 0.186535 | 0.169398 | 0.311238 | 0.208553 | 0.292838 | 0.432429 | . min | 18.000000 | 0.000000 | -3058.000000 | 0.000000 | 0.000000 | 1.000000 | 2.000000 | 1.000000 | -1.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% | 32.000000 | 0.000000 | 124.000000 | 0.000000 | 0.000000 | 8.000000 | 138.000000 | 1.000000 | -1.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . 50% | 39.000000 | 0.000000 | 553.000000 | 0.000000 | 0.000000 | 15.000000 | 253.000000 | 2.000000 | -1.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . 75% | 49.000000 | 0.000000 | 1730.000000 | 1.000000 | 0.000000 | 21.000000 | 492.000000 | 3.000000 | -1.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . max | 95.000000 | 1.000000 | 81204.000000 | 1.000000 | 1.000000 | 31.000000 | 3881.000000 | 63.000000 | 854.000000 | 41.000000 | ... | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . 8 rows × 49 columns . test.describe() . age default balance housing loan day duration campaign pdays previous ... month_jun month_mar month_may month_nov month_oct month_sep poutcome_failure poutcome_other poutcome_success poutcome_unknown . count | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | ... | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | 3349.000000 | . mean | 41.219767 | 0.016721 | 1509.294715 | 0.485219 | 0.124515 | 15.781129 | 376.676918 | 2.529113 | 54.544640 | 0.903255 | ... | 0.110182 | 0.025679 | 0.254404 | 0.077934 | 0.032846 | 0.026277 | 0.113168 | 0.054046 | 0.098835 | 0.733950 | . std | 12.061128 | 0.128245 | 3247.670769 | 0.499856 | 0.330217 | 8.422863 | 350.024934 | 2.720690 | 110.850401 | 2.620721 | ... | 0.313163 | 0.158200 | 0.435591 | 0.268107 | 0.178259 | 0.159980 | 0.316845 | 0.226142 | 0.298486 | 0.441956 | . min | 18.000000 | 0.000000 | -6847.000000 | 0.000000 | 0.000000 | 1.000000 | 4.000000 | 1.000000 | -1.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% | 32.000000 | 0.000000 | 118.000000 | 0.000000 | 0.000000 | 8.000000 | 138.000000 | 1.000000 | -1.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% | 38.000000 | 0.000000 | 546.000000 | 0.000000 | 0.000000 | 15.000000 | 259.000000 | 2.000000 | -1.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . 75% | 49.000000 | 0.000000 | 1644.000000 | 1.000000 | 0.000000 | 22.000000 | 503.000000 | 3.000000 | 85.000000 | 1.000000 | ... | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . max | 93.000000 | 1.000000 | 81204.000000 | 1.000000 | 1.000000 | 31.000000 | 3284.000000 | 43.000000 | 842.000000 | 58.000000 | ... | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . 8 rows × 49 columns . def Definedata(): # define dataset X=df_train.drop(columns=[&#39;deposit&#39;]).values y=df_train[&#39;deposit&#39;].values return X, y . def Models_NO(models, graph): import pdb model = models X, y = Definedata() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 25) model.fit(X_train,y_train) y_pred = model.predict(X_test) y_total = model.predict(X) print(y_train.sum()) if graph: train_matrix = pd.crosstab(y_train, model.predict(X_train), rownames=[&#39;Actual&#39;], colnames=[&#39;Predicted&#39;]) test_matrix = pd.crosstab(y_test, model.predict(X_test), rownames=[&#39;Actual&#39;], colnames=[&#39;Predicted&#39;]) matrix = pd.crosstab(y, model.predict(X), rownames=[&#39;Actual&#39;], colnames=[&#39;Predicted&#39;]) f,(ax1,ax2,ax3) = plt.subplots(1,3,sharey=True, figsize=(15, 2)) g1 = sns.heatmap(train_matrix, annot=True, fmt=&quot;.1f&quot;, cbar=False,annot_kws={&quot;size&quot;: 18},ax=ax1) g1.set_title(title) g1.set_ylabel(&#39;Total Deposit = {}&#39;.format(y_train.sum()), fontsize=14, rotation=90) g1.set_xlabel(&#39;Accuracy for TrainSet: {}&#39;.format(accuracy_score(model.predict(X_train), y_train))) g1.set_xticklabels([&#39;Deposit&#39;,&#39;No Deposit&#39;],fontsize=12) g2 = sns.heatmap(test_matrix, annot=True, fmt=&quot;.1f&quot;,cbar=False,annot_kws={&quot;size&quot;: 18},ax=ax2) g2.set_title(title) g2.set_ylabel(&#39;Total Deposit = {}&#39;.format(y_test.sum()), fontsize=14, rotation=90) g2.set_xlabel(&#39;Accuracy for TestSet: {}&#39;.format(accuracy_score(y_pred, y_test))) g2.set_xticklabels([&#39;Deposit&#39;,&#39;No Deposit&#39;],fontsize=12) g3 = sns.heatmap(matrix, annot=True, fmt=&quot;.1f&quot;,cbar=False,annot_kws={&quot;size&quot;: 18},ax=ax3) g3.set_title(title) g3.set_ylabel(&#39;Total Deposit = {}&#39;.format( y.sum()), fontsize=14, rotation=90) g3.set_xlabel(&#39;Accuracy for TotalSet: {}&#39;.format(accuracy_score(y_total, y))) g3.set_xticklabels([&#39;Deposit&#39;,&#39;No Deposit&#39;],fontsize=12) g3.set_ylim(2,0) # g3.set_xlim(1) plt.show() # print (&quot;&quot;) # print (&quot;Classification Report: &quot;) # print (classification_report(y, y_total)) #pdb.set_trace() else: print(&quot; t tError Table&quot;) print(&#39;Mean Absolute Error : &#39;, metrics.mean_absolute_error(y_test, (y_pred))) print(&#39;Mean Squared Error : &#39;, metrics.mean_squared_error(y_test, (y_pred) )) print(&#39;Root Mean Squared Error : &#39;, np.sqrt(metrics.mean_squared_error(y_test, (y_pred) ))) print(&#39;Accuracy on Traing set : &#39;, model.score(X_train,y_train)) print(&#39;Accuracy on Testing set : &#39;, model.score(X_test,y_test)) print(&#39;Accuracy on Testing set : &#39;, accuracy_score(y_pred, y_test)) # print(&#39;Accuracy on Testing set : &#39;, accuracy_score(y_total, y)) print(&#39;AUC score :&#39;, roc_auc_score(y, y_total)*100,&#39;%&#39;) #pdb.set_trace() return y_total, y . title = &#39;Decision Tree Regressor&#39; y_predict, y_test = Models_NO(DecisionTreeRegressor(), True) . 3691 . y_predicted, y_actual = Models_NO(DecisionTreeRegressor(), False) fpr, tpr, thresholds = roc_curve(y_actual, y_predicted) roc_auc = auc(fpr, tpr) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;,label=&#39;AUC = %0.3f&#39;% roc_auc) plt.legend(loc=&#39;lower right&#39;) plt.plot([0,1],[0,1],&#39;r--&#39;) plt.xlim([-0.1,1.0]) plt.ylim([-0.1,1.01]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() . 3691 Error Table Mean Absolute Error : 0.22096148103911614 Mean Squared Error : 0.22096148103911614 Root Mean Squared Error : 0.4700654008104789 Accuracy on Traing set : 1.0 Accuracy on Testing set : 0.11430550323827116 Accuracy on Testing set : 0.7790385189608838 AUC score : 93.34370539306866 % . title = &#39;Random Forest Classifier&#39; y_predict, y_test = Models_NO(RandomForestClassifier(), True) . 3691 . y_predicted, y_actual = Models_NO(RandomForestClassifier(), False) fpr, tpr, thresholds = roc_curve(y_actual, y_predicted) roc_auc = auc(fpr, tpr) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;,label=&#39;AUC = %0.3f&#39;% roc_auc) plt.legend(loc=&#39;lower right&#39;) plt.plot([0,1],[0,1],&#39;r--&#39;) plt.xlim([-0.1,1.0]) plt.ylim([-0.1,1.01]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() . 3691 Error Table Mean Absolute Error : 0.19408778739922364 Mean Squared Error : 0.19408778739922364 Root Mean Squared Error : 0.44055395515103896 Accuracy on Traing set : 0.9923204914885447 Accuracy on Testing set : 0.8059122126007764 Accuracy on Testing set : 0.8059122126007764 AUC score : 93.58595083937288 % . title = &#39;ExtraTreesClassifier&#39; y_predict, y_test = Models_NO(ExtraTreesClassifier(), True) . 3691 . y_predicted, y_actual = Models_NO(ExtraTreesClassifier(), False) fpr, tpr, thresholds = roc_curve(y_actual, y_predicted) roc_auc = auc(fpr, tpr) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;,label=&#39;AUC = %0.3f&#39;% roc_auc) plt.legend(loc=&#39;lower right&#39;) plt.plot([0,1],[0,1],&#39;r--&#39;) plt.xlim([-0.1,1.0]) plt.ylim([-0.1,1.01]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() . 3691 Error Table Mean Absolute Error : 0.21379516273514482 Mean Squared Error : 0.21379516273514482 Root Mean Squared Error : 0.4623798900635113 Accuracy on Traing set : 1.0 Accuracy on Testing set : 0.7862048372648551 Accuracy on Testing set : 0.7862048372648551 AUC score : 93.50666822868895 % . title = &#39;GradientBoostingClassifier&#39; y_predict, y_test = Models_NO(GradientBoostingClassifier(), True) . 3691 . y_predicted, y_actual = Models_NO(GradientBoostingClassifier(), False) fpr, tpr, thresholds = roc_curve(y_actual, y_predicted) roc_auc = auc(fpr, tpr) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;,label=&#39;AUC = %0.3f&#39;% roc_auc) plt.legend(loc=&#39;lower right&#39;) plt.plot([0,1],[0,1],&#39;r--&#39;) plt.xlim([-0.1,1.0]) plt.ylim([-0.1,1.01]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() . 3691 Error Table Mean Absolute Error : 0.16363093460734549 Mean Squared Error : 0.16363093460734549 Root Mean Squared Error : 0.4045132069628203 Accuracy on Traing set : 0.8662485600921541 Accuracy on Testing set : 0.8363690653926545 Accuracy on Testing set : 0.8363690653926545 AUC score : 85.79682468427883 % . title = &#39;KNeighborsClassifier&#39; y_predict, y_test = Models_NO(KNeighborsClassifier(), True) . 3691 . y_predicted, y_actual = Models_NO(KNeighborsClassifier(), False) fpr, tpr, thresholds = roc_curve(y_actual, y_predicted) roc_auc = auc(fpr, tpr) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;,label=&#39;AUC = %0.3f&#39;% roc_auc) plt.legend(loc=&#39;lower right&#39;) plt.plot([0,1],[0,1],&#39;r--&#39;) plt.xlim([-0.1,1.0]) plt.ylim([-0.1,1.01]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() . 3691 Error Table Mean Absolute Error : 0.2639593908629442 Mean Squared Error : 0.2639593908629442 Root Mean Squared Error : 0.5137697839139085 Accuracy on Traing set : 0.820427492640471 Accuracy on Testing set : 0.7360406091370558 Accuracy on Testing set : 0.7360406091370558 AUC score : 79.3873856141418 % .",
            "url": "https://genghua-chen.github.io/Genghua-Blog/2021/02/20/Data-Analysis-Bank-Marketing.html",
            "relUrl": "/2021/02/20/Data-Analysis-Bank-Marketing.html",
            "date": " • Feb 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Graduate Admission Data Visualization",
            "content": "import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline . Input Data . df1 = pd.read_csv(&quot;/Users/genghua/Desktop/Courses/Python/Final/archive/Admission_Predict.csv&quot;) . df1.head() . Serial No. GRE Score TOEFL Score University Rating SOP LOR CGPA Research Chance of Admit . 0 | 1 | 337 | 118 | 4 | 4.5 | 4.5 | 9.65 | 1 | 0.92 | . 1 | 2 | 324 | 107 | 4 | 4.0 | 4.5 | 8.87 | 1 | 0.76 | . 2 | 3 | 316 | 104 | 3 | 3.0 | 3.5 | 8.00 | 1 | 0.72 | . 3 | 4 | 322 | 110 | 3 | 3.5 | 2.5 | 8.67 | 1 | 0.80 | . 4 | 5 | 314 | 103 | 2 | 2.0 | 3.0 | 8.21 | 0 | 0.65 | . Data Cleaning . #Check null value datac = df1.isnull().sum() datac . Serial No. 0 GRE Score 0 TOEFL Score 0 University Rating 0 SOP 0 LOR 0 CGPA 0 Research 0 Chance of Admit 0 dtype: int64 . D = df1.drop(&#39;Serial No.&#39;, axis=&#39;columns&#39;) D . GRE Score TOEFL Score University Rating SOP LOR CGPA Research Chance of Admit . 0 | 337 | 118 | 4 | 4.5 | 4.5 | 9.65 | 1 | 0.92 | . 1 | 324 | 107 | 4 | 4.0 | 4.5 | 8.87 | 1 | 0.76 | . 2 | 316 | 104 | 3 | 3.0 | 3.5 | 8.00 | 1 | 0.72 | . 3 | 322 | 110 | 3 | 3.5 | 2.5 | 8.67 | 1 | 0.80 | . 4 | 314 | 103 | 2 | 2.0 | 3.0 | 8.21 | 0 | 0.65 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | . 395 | 324 | 110 | 3 | 3.5 | 3.5 | 9.04 | 1 | 0.82 | . 396 | 325 | 107 | 3 | 3.0 | 3.5 | 9.11 | 1 | 0.84 | . 397 | 330 | 116 | 4 | 5.0 | 4.5 | 9.45 | 1 | 0.91 | . 398 | 312 | 103 | 3 | 3.5 | 4.0 | 8.78 | 0 | 0.67 | . 399 | 333 | 117 | 4 | 5.0 | 4.0 | 9.66 | 1 | 0.95 | . 400 rows × 8 columns . df = D.rename(columns={&#39;SOP&#39;: &#39;Statement of Purpose&#39;, &#39;CGPA&#39;: &#39;GPA&#39;, &#39;LOR &#39;: &#39;Letter of Recommendation&#39;, &#39;Chance of Admit &#39;: &#39;Chance of Admit&#39;}) df . GRE Score TOEFL Score University Rating Statement of Purpose Letter of Recommendation GPA Research Chance of Admit . 0 | 337 | 118 | 4 | 4.5 | 4.5 | 9.65 | 1 | 0.92 | . 1 | 324 | 107 | 4 | 4.0 | 4.5 | 8.87 | 1 | 0.76 | . 2 | 316 | 104 | 3 | 3.0 | 3.5 | 8.00 | 1 | 0.72 | . 3 | 322 | 110 | 3 | 3.5 | 2.5 | 8.67 | 1 | 0.80 | . 4 | 314 | 103 | 2 | 2.0 | 3.0 | 8.21 | 0 | 0.65 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | . 395 | 324 | 110 | 3 | 3.5 | 3.5 | 9.04 | 1 | 0.82 | . 396 | 325 | 107 | 3 | 3.0 | 3.5 | 9.11 | 1 | 0.84 | . 397 | 330 | 116 | 4 | 5.0 | 4.5 | 9.45 | 1 | 0.91 | . 398 | 312 | 103 | 3 | 3.5 | 4.0 | 8.78 | 0 | 0.67 | . 399 | 333 | 117 | 4 | 5.0 | 4.0 | 9.66 | 1 | 0.95 | . 400 rows × 8 columns . Data Analysis . df.describe().loc[[&#39;mean&#39;, &#39;min&#39;, &#39;max&#39;]] . GRE Score TOEFL Score University Rating Statement of Purpose Letter of Recommendation GPA Research Chance of Admit . mean | 316.8075 | 107.41 | 3.0875 | 3.4 | 3.4525 | 8.598925 | 0.5475 | 0.72435 | . min | 290.0000 | 92.00 | 1.0000 | 1.0 | 1.0000 | 6.800000 | 0.0000 | 0.34000 | . max | 340.0000 | 120.00 | 5.0000 | 5.0 | 5.0000 | 9.920000 | 1.0000 | 0.97000 | . fig, ax = plt.subplots(figsize=(5,5)) df[&#39;University Rating&#39;].value_counts().plot.pie(explode = [0.1,0.1,0.1,0.1,0.1], autopct=&#39;%1.1f%%&#39;, shadow = True) ax.set_title(&#39;University Rating&#39;) ax.set_ylabel(&#39; &#39;) plt.show() . df[[&#39;GPA&#39;, &#39;University Rating&#39;]].boxplot(by=&#39;University Rating&#39;, figsize=(10,6)) plt.title(&#39;GPA vs. University Rating&#39;) plt.xlabel(&#39;University Rating&#39;) plt.ylabel(&#39;GPA&#39;) plt.show() . Quesiton 1: What will increase the chances of admission? . Question 2: Does the student who is academically good, also doing well in GRE and TOEFL? . sns.pairplot(data = df,vars=[&quot;GRE Score&quot;, &quot;TOEFL Score&quot;, #&quot;University Rating&quot;, &quot;Statement of Purpose&quot;, &quot;Letter of Recommendation&quot;, &quot;GPA&quot;, #&quot;Research&quot;, &quot;Chance of Admit&quot;]) plt.show() . Question 3: Does research paper really having a good impact in admission? . fig, ax = plt.subplots(figsize=(5,5)) df[&#39;Research&#39;].value_counts().plot.pie(explode = [0, 0.1],autopct=&#39;%1.1f%%&#39;, shadow = True) ax.set_title(&#39;Students Research&#39;) ax.set_ylabel(&#39; &#39;) plt.show() #categorical plot sns.catplot(x= &#39;Research&#39;,y = &#39;Chance of Admit&#39;, data =df) plt.show() #Scatter plot(no need) # fig, ax = plt.subplots() # a = ax.scatter(df[&#39;Research&#39;], # df[&#39;Chance of Admit&#39;]) # ax.set_xlabel(&quot;Research&quot;) # ax.set_ylabel(&quot;Chance of Admit&quot;) # plt.show() . Question 4: Who applied to the top University? . df2= df[df[&#39;University Rating&#39;]== 5] df2.describe().loc[[&#39;mean&#39;, &#39;min&#39;, &#39;max&#39;]] . GRE Score TOEFL Score University Rating Statement of Purpose Letter of Recommendation GPA Research Chance of Admit . mean | 328.333333 | 113.666667 | 5.0 | 4.5 | 4.358333 | 9.291167 | 0.866667 | 0.888167 | . min | 303.000000 | 103.000000 | 5.0 | 3.0 | 3.000000 | 8.480000 | 0.000000 | 0.610000 | . max | 340.000000 | 120.000000 | 5.0 | 5.0 | 5.000000 | 9.910000 | 1.000000 | 0.970000 | . s = pd.Series([0.000001,0.000001,0.000001,0.000001], index = [1,1.5,2,2.5]) #Statement of Purpose df3 = df2[&#39;Statement of Purpose&#39;].value_counts().sort_index() df4 = df3.append(s) #Letter of Recommendation df5 = df2[&#39;Letter of Recommendation&#39;].value_counts().sort_index() df6 = df5.append(s) . # Histogram for Statement of Purpose df4.sort_index().plot(kind=&#39;bar&#39;,figsize=(14,6), color = &#39;g&#39;) # plt.xlim =0 plt.title(&#39;Statement of Purpose with 5 University Rating&#39;) plt.xlabel(&#39;Statement of Purpose&#39;) plt.ylabel(&#39;Count&#39;) plt.show() # Histogram for Letter of Recommendation df6.sort_index().plot(kind=&#39;bar&#39;, figsize=(14,6)) plt.title(&#39;Letter of Recommendation with 5 University Rating&#39;) plt.xlabel(&#39;Letter of Recommendation&#39;) plt.ylabel(&#39;Count&#39;) plt.show() . # GPA vs. Chance of Admit fig, ax = plt.subplots(figsize = (10,5)) ax.scatter(df2[&#39;Chance of Admit&#39;], df2[&#39;GPA&#39;]) ax.set_xlabel(&quot;Chance of Admit&quot;) ax.set_ylabel(&quot;GPA&quot;) plt.show() #GRE Score vs. Chance of Admit fig, ax = plt.subplots(figsize = (10,5)) ax.scatter(df2[&#39;Chance of Admit&#39;], df2[&#39;GRE Score&#39;]) ax.set_xlabel(&quot;Chance of Admit&quot;) ax.set_ylabel(&quot;GRE Score&quot;) plt.show() . Conclusion . # corr # Example # &gt;&gt;&gt; corr = np.corrcoef(np.random.randn(10, 200)) # &gt;&gt;&gt; mask = np.zeros_like(corr) # &gt;&gt;&gt; mask[np.triu_indices_from(mask)] = True # &gt;&gt;&gt; with sns.axes_style(&quot;white&quot;): # ... f, ax = plt.subplots(figsize=(7, 5)) # ... ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True) corr = df.corr() mask = np.zeros_like(corr) mask[np.triu_indices_from(mask)] = True with sns.axes_style(&quot;white&quot;): f, ax = plt.subplots(figsize=(15, 15)) ax = sns.heatmap(corr,mask=mask,square=True, annot=True,linewidths=.8,cmap=&quot;YlGnBu&quot;) #Last row cut in half of heatmap plot ax.set_ylim(8,0) .",
            "url": "https://genghua-chen.github.io/Genghua-Blog/2020/12/01/Graduate-Admission.html",
            "relUrl": "/2020/12/01/Graduate-Admission.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "EDUCATION Feitian College at Middletown, Middletown, NY Expected June 2022 Bachelor of Data Science Stratford High School, Houston, TX May 2018 .",
          "url": "https://genghua-chen.github.io/Genghua-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://genghua-chen.github.io/Genghua-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}